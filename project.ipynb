{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2017\n",
    "\n",
    "#### PROJECT\n",
    "\n",
    "# SCUPER: Social Circle and User PErsonality based Recommendation System\n",
    "\n",
    "### Team : Aniket Bonde, Nagaraj Janakiraman, Sudheer Dantuluri\n",
    "\n",
    "### [25% of final grade]\n",
    "\n",
    "### Due: Tuesday, May 2 by 11:59pm\n",
    "\n",
    "*Goal of this project:* Get hand-on experience with building a recommendation system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction and Problem Statement\n",
    "\n",
    " Recommender systems play a pivotal role in boosting the sales of major e-commerce systems. With rapid increase in the number of registered users and products on e-commerce systems, the problem of cold start for users (new users into the RS with little historical behavior) and the sparsity of datasets (the proportion of rated user-item pairs in all the user-item pairs of RS) has become increasingly intractable.\n",
    "\n",
    " Fortunately, the appearance of web2.0 greatly improved user’s initiative on the Internet, and then brought volume of social networks such as Facebook, Twitter, Yelp, Douban, Epinions, etc. This new factor of social network - the interpersonal interest based on circles of friends brings opportunities to the recommender system to solve the cold start & sparsity problem of datasets.\n",
    "\n",
    " In this Project, we propose to implement a personalized recommendation model that fuses in user’s personal interest and social factors like interpersonal similarity and interpersonal influence that is based on social network. The factor of user personal interest captures direct connection between user and item latent feature vectors, and social factors capture connections between user and his/here friend's latent feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Related Work\n",
    "\n",
    " Recently, there have been some works that address to solve the cold start problem by factoring in social network features. Some recent significant contributions are CircleCon Model [1], ContextMF model [2] and [3]. [1] proposed a model that uses the concept of ‘inferred trust circle’ based on the domain-obvious circles of friends on social network. However, [2] demonstrated that individual preference (personal interest) is also a significant factor in social network. [3] builds on these ideas and proposes a personalized recommendation model based on probabilistic matrix factorization that considers three factors - personal interest, interpersonal interest similarity and interpersonal influence. Detailed explanation for each factor is presented in the high-level-approach section.\n",
    "\n",
    " In our project, we desire to understand the effects of factors proposed in [3], and evaluate it on a real dataset like ‘Yelp’ to recommend businesses for users with very less rating history (cold start scenario). We would evaluate the model that considers user personal interest and social circle and compare it against a model like classic Matrix Factorization that neglects factors from social network.\n",
    "        \n",
    "### 2.1 References\n",
    "\n",
    "[1] X.-W. Yang, H. Steck, and Y. Liu, “Circle-based recommendation in online social networks,” in Proc. 18th ACM SIGKDD Int. Conf. KDD, New York, NY, USA, Aug. 2012, pp. 1267–1275.\n",
    "\n",
    "[2] M. Jiang et al., “Social contextual recommendation,” in Proc. 21st ACM Int. CIKM, New York, NY, USA, 2012, pp. 45–54.\n",
    "\n",
    "[3] H. Feng and X. Qian, “Recommendation via user’s personality and social contextual,” in Proc. 22nd ACM CIKM, New York, NY, USA, 2013\n",
    "\n",
    "[4] R. Sinha and K. Swearingen, “Comparing recommendations made by online systems and friends,” in Proc. DELOS Workshop Personalisation Recommender Systems Digital Libraries, Dublin, Ireland, 2001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. High-level Approach\n",
    "\n",
    "  We first start with the basic matrix factorization model without any social factors. The task of a recommender system is to decrease the error of predicted value and the actual rating. Thus, this basic model is trained on the observed rating data by minimizing the objective error function (using gradient descent)\n",
    " \n",
    "![title](PRM.jpg)\n",
    "\n",
    "  Then we add three factors proposed in PRM for better personalization:\n",
    "\n",
    "## 3.1 Interpersonal influence: Whom you would trust?\n",
    "    \n",
    "  Interpersonal influence is the measure of trust a particular user puts in another user. The results in [4] show that the user’s friends consistently provided better recommendations. For example, 90% of users believe the book recommended is good from friends, 75% of users believe that the recommendation is useful from friends.\n",
    "\n",
    "## 3.2 Interpersonal interest similarity: Whose interest is similar to yours?\n",
    "\n",
    "   The basic idea is that user latent feature vector should be similar to his/her friends’ latent feature vector based on the similarity of their interest. Also, for cold start users, we infer interest circle to enhance the intrinsic link of user latent feature.\n",
    "\n",
    "## 3.3 User personal interest: Which items you would interest in?\n",
    "\n",
    " Personal interest denotes user’s individuality of rating items, especially for the experienced users, and these factors fuse together to improve the accuracy and applicability of recommender system.\n",
    "    \n",
    " The model that includes the 3 factors is again trained on the training dataset to minimize an objective cost function (using gradient descent) to learn the best latent features.\n",
    " \n",
    " We then use the latent features to predict the ratings for the testing set.  \n",
    " \n",
    "## 3.4 Flow-chart\n",
    "\n",
    "![title](flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Methodology\n",
    "\n",
    "## 4.1 Dataset\n",
    "\n",
    "Initially, we chose to use the [yelp](https://www.yelp.com/dataset_challenge) data-set  in our proposal. We started our project by trying to come up with a simplified dataset first. Couple of strategies that we tried include:\n",
    "\n",
    "a. Finding user with maximum number of friends and then expanding the user-database by crawling breadth first to n(initialized to 1) levels from this user. We stop incrementing n if the size of the accumulated number of users crosses our target number of users. Then we consider only reviews and businesses that correspond to these users only.\n",
    "\n",
    "b. Pick target/10 random users and expand them one level.Then we consider only reviews and businesses that correspond to these users only.\n",
    "\n",
    "Code for the above experiments present in github @ dataset/baby/compressDataset.py and read_json.py.\n",
    "\n",
    "However, while evaluating various strategies in trying to come up with a smaller, \"proper\" (one that preserves the social relations while trimming the dataset), we came across [this](http://smiles.xjtu.edu.cn/Download/Download_yelp.html) processed dataset. It is composed from the original yelp data-set by the very authors of the research paper that this project is based up on.\n",
    "\n",
    "The authors have created these processed datasets for the following eight categories : restaurants, shopping, nightlife, pets, hotelstravel, homeservices, beautysvc and active. Among those, we chose the dataset \"Night life\" as we found the size of that dataset to be well suited for our task given the constraint on the resources (time and computational-power).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.2 Algorithm\n",
    "\n",
    "### 4.2.1 Notation\n",
    "\n",
    "D - the topic distribution vector.\n",
    "\n",
    "Q - the relevance matrix of users u's interest to the topic of item i.\n",
    "\n",
    "W - the interest similarity matrix of users u to user v.\n",
    "\n",
    "S - the matrix of user u trust on v.\n",
    "\n",
    "H - the set of items rated by users.\n",
    "\n",
    "U - user latent feature vector matrix.\n",
    "\n",
    "P - item latent feature vector matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "import scipy\n",
    "from sklearn.preprocessing import normalize\n",
    "from itertools import cycle\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "items_path = r'one_category_dataset/items.txt'\n",
    "users_path = r'one_category_dataset/users.txt'\n",
    "ratings_train_path = r'one_category_dataset/nightlife_training.txt'\n",
    "ratings_test_path = r'one_category_dataset/nightlife_test.txt'\n",
    "subcategories_path = r'one_category_dataset/subcategories_list.txt'\n",
    "\n",
    "\n",
    "eps = 10               # epsilon \n",
    "\n",
    "#mode = 0               # Simple MF model (No social graph)  \n",
    "#mode = 1               #  UI (Q) + MF\n",
    "#mode = 2               #  II (S) + MF\n",
    "mode = 3               #  IS (W) + MF\n",
    "# mode = 4               #  UI + II  (Q,S)\n",
    "# mode = 5               #  UI + IS  (Q,W)\n",
    "# mode = 6               #  II + IS  (S,W)\n",
    "# mode = 7               #  ALL   (Q,S,W)\n",
    "\n",
    "print 'Mode:', mode\n",
    "\n",
    "items_df = pd.read_table(items_path, delimiter = r'::', engine='python')\n",
    "users_df = pd.read_table(users_path, delimiter = r':', engine='python')\n",
    "\n",
    "#ratings_df = pd.read_table(ratings_path, delimiter = r'::', engine='python')\n",
    "\n",
    "ratings_train_df = pd.read_table(ratings_train_path, delimiter = r'::', engine='python')\n",
    "ratings_test_df = pd.read_table(ratings_test_path, delimiter = r'::', engine='python')\n",
    "\n",
    "items = len(items_df)\n",
    "users = len(users_df)\n",
    "\n",
    "f = open(subcategories_path)\n",
    "subcategories = [x.strip() for x in f.readlines()]\n",
    "                 \n",
    "def friends_str2list(str_friends):\n",
    "    return map(int, re.sub(r'{', '', str_friends).split(',')[:-1])\n",
    "\n",
    "def space2underscore(string):\n",
    "    return re.sub(r' ', '_', string)\n",
    "    \n",
    "def topic_dist(sub_category_list, categories):\n",
    "    tpc_dist = [0]*len(categories)\n",
    "    for i in range(len(tpc_dist)):\n",
    "        if (categories[i] in sub_category_list):\n",
    "            tpc_dist[i] = 1\n",
    "    return tpc_dist\n",
    "       \n",
    "    \n",
    "users_df['friends'] = map(friends_str2list, users_df['friends'])\n",
    "subcategories = map(space2underscore, subcategories)\n",
    "items_df['sub_category'] = map(space2underscore, items_df['sub_category'])\n",
    "items_df['topic_distribution'] = map(topic_dist, items_df['sub_category'], [subcategories]*(len(items_df['sub_category'])))\n",
    "#users_df['topic_distribution'] = [[0]*18]*len(users_df['topic_distribution'])\n",
    "\n",
    "nil_tpc_dist_items = []\n",
    "for i in range(len(items_df['topic_distribution'])):\n",
    "    if(items_df['topic_distribution'][i] == [0] * 18):\n",
    "        nil_tpc_dist_items.append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Build Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Building Matrices\n",
    "#m = number of users, n = number of items\n",
    "\n",
    "temp, H = [], []\n",
    "for user, group in ratings_train_df.groupby('user_id'):\n",
    "    temp.append(np.mean(list(items_df.loc[list(group['item_id'])]['topic_distribution']), axis = 0))\n",
    "    H.append(len(group))\n",
    "    \n",
    "users_df['topic_distribution'] = temp\n",
    "\n",
    "#H = np.matrix(H[0])        \n",
    "H = normalize(H[0], norm ='l1')\n",
    "\n",
    "\n",
    "# 1) Q (m * n) = Relevance matrix of user 'u' to topic of item 'i'\n",
    "\n",
    "Q = np.nan_to_num(1 - scipy.spatial.distance.cdist(list(users_df['topic_distribution']), list(items_df['topic_distribution']), 'cosine'))\n",
    "\n",
    "\n",
    "\n",
    "# 2) S (m * m) = Trust of user 'u' on user 'v'\n",
    "S = np.zeros((users, users))\n",
    "S_sym = np.zeros((users, users))\n",
    "\n",
    "for i in range(users):\n",
    "    for user, friend in zip([i] * len(users_df['friends'][i]), users_df['friends'][i]):\n",
    "        S[user, friend] = 1\n",
    "        S_sym[user,friend] = 1\n",
    "        S_sym[friend,user] = 1\n",
    "\n",
    "# 3) W (m * m) = Similarity matrix of user 'u' to topic of user 'v'\n",
    "W = np.nan_to_num(1 - scipy.spatial.distance.cdist(list(users_df['topic_distribution']), list(users_df['topic_distribution']), 'cosine'))\n",
    "W = np.multiply(S_sym,W)  \n",
    "W = normalize(W, norm='l2')  \n",
    "\n",
    "R = np.empty([len(users_df), len(items_df)], dtype = np.float16)\n",
    "I = np.empty([len(users_df), len(items_df)], dtype = np.float16)\n",
    "\n",
    "R_test = np.empty([len(users_df), len(items_df)], dtype = np.float16)\n",
    "I_test = np.empty([len(users_df), len(items_df)], dtype = np.float16)\n",
    "\n",
    "\n",
    "num_ratings_train = 0;        \n",
    "for user, item, rtng in zip(ratings_train_df['user_id'], ratings_train_df['item_id'], ratings_train_df['rating']):\n",
    "    R[user, item] = rtng\n",
    "    I[user, item] = 1\n",
    "    num_ratings_train +=1;\n",
    "\n",
    "    \n",
    "num_ratings_test = 0;\n",
    "for user, item, rtng in zip(ratings_test_df['user_id'], ratings_test_df['item_id'], ratings_test_df['rating']):\n",
    "    R_test[user, item] = rtng\n",
    "    I_test[user, item] = 1\n",
    "    num_ratings_test +=1;\n",
    "###############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.2.3 Cost function & Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.8 Performing gradient descent (no vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.9 Performing gradient descent (vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ccd456764727>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mR_cap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcal_pred_rating\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[1;31m#print 'Rcap:', R_cap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'r' is not defined"
     ]
    }
   ],
   "source": [
    "#Gradient Descent\n",
    "\n",
    "#Parameters for Gradient Descent\n",
    "global lamda, beta, gamma, eta, l\n",
    "k = 10                                  #dimension of latent space\n",
    "lamda = 0.1\n",
    "beta = 30\n",
    "gamma = 30\n",
    "eta = 30\n",
    "l = 0.000005\n",
    "\n",
    "np.random.seed(0)\n",
    "U = 0.1 * np.random.randn(users, k)\n",
    "P = 0.1 * np.random.randn(items, k)\n",
    "\n",
    "######################################################\n",
    "#U = pickle.load( open( \"U_0.pckl\", \"rb\" ) )\n",
    "#P = pickle.load( open( \"P_0.pckl\", \"rb\" ) )\n",
    "########################################################\n",
    "\n",
    "#r = np.mean(ratings_df['rating'])\n",
    "r = np.empty([len(users_df), len(items_df)], dtype = np.float16)\n",
    "for user, rating_group in ratings_train_df.groupby('user_id'):\n",
    "    r[user][:] = np.mean(rating_group['rating'])\n",
    "###############################################################################\n",
    "\n",
    "#Gradient Descent\n",
    "\n",
    "def cal_pred_rating(r, U, P):\n",
    "    return (r + np.matmul(U, P.transpose()))\n",
    "    \n",
    "def cal_error_der_P(I_, R_, U_, H_, Q_, P_):\n",
    "    first_fac = 0\n",
    "    second_fac = 0\n",
    "    third_fac = 0\n",
    "    \n",
    "    first_fac = np.matmul(np.multiply(I_.transpose(), R_.transpose()), U_)\n",
    "    \n",
    "    second_fac = lamda * P_\n",
    "    \n",
    "    if(mode in [1,4,5,7]):\n",
    "        third_fac = eta * np.matmul(np.multiply(np.multiply(I_.transpose(), np.matlib.repmat(H_, items, 1)), \n",
    "                             (np.subtract((np.matmul(U_, P_.transpose())), Q_)).transpose()), U_)\n",
    "    \n",
    "    return first_fac + second_fac + third_fac\n",
    "\n",
    "\n",
    "R_cap = cal_pred_rating(r, U, P)\n",
    "\n",
    "#print 'Rcap:', R_cap\n",
    "\n",
    "def cal_error_der_U(I_, R_, P_, H_, Q_, U_, W_, S_):\n",
    "    \n",
    "    first_fac = 0\n",
    "    second_fac = 0\n",
    "    third_fac = 0\n",
    "    fourth_fac = 0\n",
    "    fifth_fac = 0\n",
    "    sixth_fac = 0\n",
    "    seventh_fac = 0\n",
    "    \n",
    " \n",
    "    first_fac = np.matmul(np.multiply(I_, R_), P_)\n",
    "    \n",
    "    second_fac = lamda * U_\n",
    "    \n",
    "    if(mode in [2,4,6,7]):\n",
    "        third_fac = beta * np.subtract(U_, np.matmul(S_,U_))\n",
    " \n",
    "    if(mode in [2,4,6,7]):\n",
    "        fourth_fac = -beta *  np.matmul(S_.transpose(),np.subtract(U, np.matmul(S,U))) \n",
    "    \n",
    "    if(mode in [3,5,6,7]):\n",
    "        fifth_fac = gamma * np.subtract(U_, np.matmul(W_,U_))\n",
    "    \n",
    "    if(mode in [3,5,6,7]):\n",
    "        sixth_fac = -gamma * np.matmul(W_.transpose(),np.subtract(U, np.matmul(W,U)))\n",
    "    \n",
    "    if(mode in [1,4,5,7]):\n",
    "        seventh_fac = eta * np.matmul(np.multiply(np.multiply(I_, (np.matlib.repmat(H_, items, 1)).transpose()), \n",
    "                             np.subtract((np.matmul(U_, P_.transpose())), Q_)), P_)\n",
    "        \n",
    "    return first_fac + second_fac + third_fac + fourth_fac + fifth_fac + sixth_fac +seventh_fac\n",
    "    \n",
    "#error_der_U = map(cal_error_der_U, I, (R_cap - R), [P] * users, H.transpose(), Q, U)\n",
    "\n",
    "def cal_error_fn(R, R_cap, H, Q, U, P, S, W, I):\n",
    "    \n",
    "    first_fac = 0\n",
    "    second_fac = 0\n",
    "    third_fac = 0\n",
    "    fourth_fac = 0\n",
    "    fifth_fac = 0\n",
    "    \n",
    "    \n",
    "    first_fac = np.sum(np.sum(np.multiply(R - np.multiply(R_cap,I), R - np.multiply(R_cap,I)), axis = 1), axis = 0) / float(2)\n",
    "                      \n",
    "    second_fac = (lamda/float(2)) *(np.linalg.norm(U,ord='fro') + np.linalg.norm(P,ord='fro'))\n",
    "    \n",
    "    if(mode in [2,4,6,7]):\n",
    "        third_fac_temp = np.subtract(U, np.matmul(S,U))\n",
    "        third_fac = (beta/float(2)) * ((np.linalg.norm(third_fac_temp,ord='fro'))**2)\n",
    "    \n",
    "    if(mode in [3,5,6,7]):\n",
    "        fourth_fac_temp = np.subtract(U, np.matmul(W,U))\n",
    "        fourth_fac = (gamma/float(2)) * ((np.linalg.norm(fourth_fac_temp,ord='fro'))**2)\n",
    "    \n",
    "    if(mode in [1,4,5,7]):\n",
    "        fifth_fac_temp = np.subtract(Q, np.matmul(U, P.transpose()))\n",
    "        fifth_fac = (eta/float(2)) * np.sum(np.sum(np.multiply((np.matlib.repmat(H, items, 1)).transpose(), np.multiply(fifth_fac_temp, fifth_fac_temp)), axis = 1), axis = 0)\n",
    "    \n",
    "        \n",
    "    #print 'first:',first_fac , 'second:', second_fac, 'fifth:', fifth_fac\n",
    "    \n",
    "    return first_fac + second_fac + third_fac + fourth_fac + fifth_fac\n",
    "\n",
    "print(cal_error_fn(R, R_cap, H, Q, U, P, S,W,I))\n",
    "del users_df\n",
    "del items_df\n",
    "del ratings_train_df\n",
    "\n",
    "\n",
    "U = U.astype(dtype = np.float16)\n",
    "P = P.astype(dtype = np.float16)\n",
    "\n",
    "t=0\n",
    "\n",
    "identifier = (np.arange(users)).transpose() \n",
    "\n",
    "Error_train_old = 100000\n",
    "Error_test_old = 100000\n",
    "\n",
    "while(t<100):\n",
    "        print t\n",
    "        error_der_U = cal_error_der_U(I, (R_cap - R), P, H, Q, U, W, S)        \n",
    "        error_der_P = cal_error_der_P(I, (R_cap - R), U, H, Q, P)\n",
    "        \n",
    "        U = np.subtract(U, np.multiply(l, error_der_U))\n",
    "        \n",
    "        for i in range(users):\n",
    "            for j in range(k):\n",
    "                P[i][j] -= (l * error_der_P[i][j])\n",
    "                \n",
    "        R_cap = cal_pred_rating(r, U, P)\n",
    "        \n",
    "         \n",
    "        print 'before RMSE' \n",
    "        RMSE_test = np.sqrt(np.sum(np.square(np.subtract(np.multiply(I_test,R_cap),R_test)))/float(num_ratings_test))\n",
    "        RMSE_train = np.sqrt(np.sum(np.square(np.subtract(np.multiply(I,R_cap),R)))/float(num_ratings_train))\n",
    "        \n",
    "        \n",
    "        print 'RMSE_train:', RMSE_train, 'RMSE_test', RMSE_test\n",
    "        #print R_cap\n",
    "        \n",
    "        Error_train = cal_error_fn(R, R_cap, H, Q, U, P,S,W,I)\n",
    "        Error_test = cal_error_fn(R_test, R_cap, H, Q, U, P,S,W,I_test)\n",
    "        \n",
    "        print 'Error_train:', Error_train ,'Error_test:', Error_test\n",
    "        \n",
    "        if t>0:\n",
    "            \n",
    "            if (Error_test-Error_test_old > 0):\n",
    "                if (Error_train-Error_train_old > 0):\n",
    "                    print 'Decrease learning rate!'\n",
    "                else:\n",
    "                   print 'Exiting because of overfitting'\n",
    "                sys.exit(0)\n",
    "                \n",
    "            if (Error_train-Error_train_old > 0):\n",
    "                print 'Decrease learning rate!'\n",
    "                sys.exit(0)\n",
    "                \n",
    "            if ((Error_train_old-Error_train)<eps):\n",
    "                print 'Converged!'\n",
    "            #    sys.exit(0)    \n",
    "                \n",
    "        Error_train_old = Error_train\n",
    "        Error_test_old = Error_test\n",
    "        \n",
    "        t +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation\n",
    " \n",
    "## 5.1 Metric\n",
    "\n",
    " We evaluate our model by comparing the two models (with and without social interaction information) based on Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). We also evaluate our models for users that have very less rating history (cold start scenario) and draw conclusions on the effect of using social features.\n",
    " \n",
    "## 5.2 Results\n",
    "\n",
    "### 5.2.1 PRM Model - Iteration vs training-error & test-error\n",
    "\n",
    "Graph-plot Curve\n",
    "\n",
    "Write Key-takeaways\n",
    "\n",
    "### 5.PRM Model - Bar graph comparing RMSE of MF, UI, IS, II, PRM\n",
    "\n",
    "bar-graph\n",
    "\n",
    "Write Key-takeaways\n",
    "\n",
    "### PRM Model - Bar graph comparing MAE of MF, UI, IS, II, PRM\n",
    "\n",
    "bar-graph\n",
    "\n",
    "Write Key-takeaways\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion and Next Steps\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this poject, a personalized recommendation approach was implemented by combining social network factors: personal interest, interpersonal interest similarity, and interpersonal influence. In particular, the personal interest denotes user’s individuality of rating items, especially for the experienced users, and these factors were fused together to improve the accuracy and applicability of recommender system. \n",
    "\n",
    "\n",
    "Experiments were performed on yelp \"Night-life\" processed dataset and it was observed that the model that fuses in social network information along with users personal interest shows significant improvements over approaches that\n",
    "do not factor in social network information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
